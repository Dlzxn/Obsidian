source: [[Программирование/BIG DATA/BIG DATA|BIG DATA]]
tegs: #bigdata

Когда мы говорим, про обработку обычных данных, то все ненароком думают про SQL и Pandas. Про них мы также рассказываем на курсе. Но, что если я Вам скажу, что на обычном "железе" можно обрабатывать и большие данные? Да, это так! Понятно, что не быстро, но можно. То есть я думаю, что Вы догадались, что все дело в архитектуре вычислений. Рассмотрим список инструментов, которым можно обрабатывать большие данные.

1. **Распределенные вычисления:**
    
    - **Apache Hadoop:**
        - Платформа для распределенной обработки больших данных, которая включает в себя HDFS и MapReduce. MapReduce позволяет разбивать задачи на небольшие подзадачи, которые выполняются параллельно на разных узлах кластера, а затем собираются в единый результат. **На курсе мы знакомимся с MapReduce только в теории, так как технология уже устарела.**
    - **Apache Spark:**
        - Мощная платформа для распределенных вычислений, которая обеспечивает более быструю обработку данных по сравнению с Hadoop благодаря использованию in-memory вычислений (и на самом деле не только). Spark поддерживает разнообразные задачи: от обработки потоков данных (Streaming) до машинного обучения (MLlib). Тут стоить добавить, что во-первых мы проходим Spark углубленно, а во-вторых, а что такое Streaming, спросите Вы. Спешу Вас обрадовать, что об этом подготовлен следующий слайд.
2. **Потоковая обработка данных:**
    
    - **Apache Kafka:**
        - Платформа для обработки потоков данных в реальном времени, которая позволяет обрабатывать большие объемы данных, поступающих с высокой скоростью. Kafka используется для сбора, хранения и обработки потоков данных, таких как события IoT или логи сервера. Но на самом деле обработку именно самих данных кафка не делает. Она их перенаправляет.
    - **Apache Flink, Apache Storm:**
        - Инструменты для потоковой обработки данных, которые обеспечивают низкую задержку и возможность обработки данных в реальном времени. Flink и Storm используются для задач, требующих мгновенного отклика на поступающие данные. На курсе, к сожалению, данные инструменты не проходятся. Они являются аналогами Kafka.
3. **Инструменты для анализа данных:**
    
    - **Hive:**
        - Инструмент, работающий поверх Hadoop, который позволяет выполнять SQL-подобные запросы к данным, хранящимся в HDFS. Hive упрощает анализ больших данных, предоставляя интерфейс, знакомый пользователям SQL. На курсе этому посвящена отдельная глава.
    - **Presto:**
        - Движок для выполнения SQL-запросов по распределённым данным, который обеспечивает высокую производительность и возможность работы с данными, хранящимися в разных системах (HDFS, S3, реляционные базы данных).
4. **Облачные платформы для обработки больших данных:**
    
    - **Google Cloud BigQuery:**
        - Инструмент для обработки больших данных, который позволяет выполнять SQL-запросы по большим наборам данных, хранящимся в облаке. BigQuery обеспечивает высокую производительность и масштабируемость, позволяя обрабатывать петабайты данных за считанные секунды.
    - **Amazon EMR (Elastic MapReduce):**
        - Облачная платформа от Amazon для обработки больших данных, основанная на Apache Hadoop и Apache Spark. EMR позволяет быстро разворачивать кластеры и обрабатывать большие объемы данных в облаке.
    - **Microsoft Azure Synapse Analytics:**
        - Облачная аналитическая платформа, которая объединяет хранение данных и их обработку, обеспечивая возможность работы с большими данными с использованием SQL и Spark.

Конечно, большие данные можно обрабатывать Python-ом, пандасом и так далее. Но опять же, сколько времени нам потребуется? А время - это самый главный ресурс! Двигаемся дальше и наводим порядок у себя в головах!

![](https://ucarecdn.com/5ca81b72-8c77-449d-b81a-317a21849828/)